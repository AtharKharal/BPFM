---
title: "R Solution"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(huxtable)
library(SmartEDA)
library(mlr3verse)
library(mlr3learners)
library(mlr3viz)
```
# Data Cleaning  
  
## Original Dataset  

A glimpse into 100 random rows of the dataset is given below:  

```{r}
dt_original  <- read.csv("Blood_Pressure_data.csv", stringsAsFactors=TRUE)
DT::datatable(dt_original[sample(nrow(dt_original),100),])
```
  
### Structure (Original)  
  
Structure of the dataset is given below:

```{r}
glimpse(dt_original)
dt_original[dt_original=="?"] <- NA
```

## Health of Variables  

Dataset health may be seen by following table:

```{r}
skm_original <- skimr::skim(dt_original)
skm_original %>% filter(complete_rate<0.999999) %>% 
  select(skim_variable, n_missing, complete_rate, factor.n_unique) %>% 
  rename(Variable=skim_variable,Missings=n_missing,CompletenessRate=complete_rate,Uniques=factor.n_unique) %>% 
  arrange(CompletenessRate) %>% 
  as_huxtable(add_colnames=T) %>% set_top_border(row=2, value = 0.4)
```

## Cleaning Tasks  
On the basis of above data health table and other Machine Learning considerations we drop certain columns and also apply a transformation on label variable. Details of these cleaning steps are as follows:  

- Replace all "?" with NA (Not Available).  

- Variables: *id, weight, discharge_disposition_id, admission_source_id, patient_no and admission_typeid* are useless because of having more than 10 factors   
- Variables *payer_code and medical_specialty* have very high number of factors 17 and 72, respectively.  Moreover they contain integer and alphanumeric factors simultaneously.  

- *examide and citoglipton* have only 1 factor throughout i.e. have 0 variance  

- Factors '>5' and '<30' have been lumped together in new category named 'YES'.

- Variable age.group has been designated as an ordinal categorical variable.  

## Cleaned Dataset  

```{r}
dt <- dt_original %>% 
  select(-c(id, weight, discharge_disposition_id, admission_source_id, patient_no, admission_typeid)) %>%   # useless cols e.g. Id's and having more than 10 factors
  select(-c(payer_code, medical_specialty)) %>%   # high cardinality factor variables respectively 17 and 72
  select(-c(diag_1, diag_2, diag_3)) %>%   # useless because these contain integer and V## type mixed entries, factors are more than 700
  select(-c(examide,citoglipton)) %>%      # have only 1 factor throughout 
  mutate(label=fct_recode(dt_original$label,YES=">5",YES="<30"),
         age.group=factor(age.group,ordered=T)
         )
```



## Class Balance  

It is an almost balanced dataset in terms of classes:
```{r}
table(dt$label) %>% as.data.frame() %>% rename(Label=Var1) %>%  as_huxtable(add_colnames=T) %>% set_top_border(row=2, value = 0.4)
```
  
Difference of 7962 between YES and NO is negligible because the total available labels are 101766.  

# Exploratory Data Analysis  
  
## High Frequency Visitors  

Certain patients are abnormally high visitors e.g. 1 patient with Patient No. 88785891 has visited 40 times. Other visit frequencies are given below (for example first entry is to be read as "717 patients visited 5 times")
```{r}
vv=sort(table(dt_original$patient_no),decreasing = T)
mm <- sort(table(vv[vv>4]),decreasing=T) %>% as.data.frame() %>% as.matrix() 
dimnames(mm) <- list(c(),c('Visits', 'No of Patients'))
mm %>% as_huxtable(add_colnames=T) %>% set_top_border(row = 2)
```

## Vislaizations for Factor Variables vs. Target Variables  
 Following visualizations help to decide visually about the variation present in each variable and hence its elimination or otherwise. As a result of these visualizations we decided to delete following variables:  
 
* repaglinide, nateglinide, chlorpropamide, acetohexamide, tolbutamide, acarbose, miglitol, troglitazone, tolazamide, glyburide, glipizide, glimepiride.pioglitazone, metformin, metformin.pioglitazone, metformin.rosiglitazone,  *
```{r}
 ExpCatViz(dt, target = NULL, 
           col=c("springgreen1", "red1")
           )
```
```{r}
dt <- dt %>% select(-c(repaglinide, nateglinide, chlorpropamide, acetohexamide, tolbutamide, acarbose, miglitol, troglitazone, tolazamide, glyburide, glipizide, glimepiride.pioglitazone, metformin, metformin.pioglitazone, metformin.rosiglitazone, glipizide.metformin))
```

### Structure (Cleaned)  

Structure of the cleaned dataset is given below:

```{r}
skimr::skim(dt)
```
A glimpse of the actual values afer all cleaning is as follows:  

```{r}
glimpse(dt)
```


## Vislaizations for Numeric Variables vs. Target Variables  

Three numeric variables appear to be of less variation namely: *number_outpatient , number_emergency, number_inpatient*. But we decide to keep these for the time being as we already have a smaller number of numeric variables i.e. 8.
```{r}
 ExpNumViz(dt, target = 'label', 
           type =2, 
           nlim=3,
           col=c("springgreen1", "red1")
           )
```
  
```{r}
# write_csv(dt,"dt_cleaned.csv")
# saveRDS(dt,'dt_cleaned.R')
```
  
# Model Development  

```{r}
# Make a task 
#dt = read_rds('dt_cleaned.R')
dt = read_csv('dt_cleaned.csv',col_types = 'fffiiiiiiiiffffffffff' )  # 'fffiiiiiiiiffffffffff'
# dt <- dt %>% mutate(age.group=factor(age.group, ordered = T))
tk <- TaskClassif$new(id = 'tsk_dt', backend = dt, target = 'label')
```

## Naive Baye's Model  

```{r}
lrn_naive_bayes = lrn("classif.naive_bayes")
lrn_naive_bayes$train(tk, row_ids = sample(nrow(dt),size = 90000) )
prd_naive_bayes = lrn_naive_bayes$predict(tk, row_ids = sample(nrow(dt), size = 1000))

lrn_naive_bayes$model
```
Performance of Naive Baye's Model is as follows: 

```{r}
print(paste0("Accuracy = ", prd_naive_bayes$score(msr("classif.acc"))))
print(paste0("F score = ", prd_naive_bayes$score(msr("classif.fbeta"))))
```

## Random Forest Model  
```{r}
lrn_ranger = lrn("classif.ranger")
lrn_ranger$train(tk, row_ids = sample(nrow(dt),size = 90000) )
prd_ranger = lrn_ranger$predict(tk, row_ids = sample(nrow(dt), size = 1000))

lrn_ranger$model

# GraphLearner$new(po('encode') %>>% lrn('classif.svm'),id = 'svm')
# GraphLearner$new(po('encode') %>>% lrn('classif.xgboost'),id = 'xgboost')
```

Performance and Confusion Matrix of Random Forest model is as follows: 
```{r}
print(paste0("Accuracy = ", prd_ranger$score(msr("classif.acc"))))
print(paste0("F score = ", prd_ranger$score(msr("classif.fbeta"))))
lrn_ranger$model$confusion.matrix
```


## Decision Trees  

```{r}
lrn_rpart = lrn('classif.rpart')
lrn_rpart$train(tk, row_ids = sample(nrow(dt),size = 90000) )
prd_rpart = lrn_rpart$predict(tk, row_ids = sample(nrow(dt), size = 1000))
lrn_rpart$model
```

Performance of Decision Tree Model is given as :  
```{r}
print(paste0("Accuracy = ", prd_rpart$score(msr("classif.acc"))))
print(paste0("F score = ", prd_rpart$score(msr("classif.fbeta"))))
```


```{r}
lrns()
msrs()
```


```{r}
# learners = list(
#    lrn('classif.kknn'), lrn('classif.log_reg'), lrn('classif.naive_bayes'),
#    lrn('classif.lda'), lrn('classif.ranger'), lrn('classif.rpart'),
#    GraphLearner$new(po('encode') %>>% lrn('classif.xgboost'),id = 'xgboost')
#  )
# 
# 
# design = benchmark_grid(tasks = tk, learners = learners,
#                         resamplings = rsmp('cv', folds = 10L))

# bmr = benchmark(design, store_models = TRUE)

# saveRDS(bmr,'benchmark_results.R')
# 
# bmr_retrieved = readRDS('benchmark_results.R')
# bmr_rslt = bmr_retrieved$aggregate(msr('classif.acc')) %>%
#   arrange(desc(classif.acc)) %>%
#   select(-c(resample_result, resampling_id, iters,nr))
# # Benchmarking results are given below:
# bmr_rslt %>% 
#   pivot_wider(id_cols = 'learner_id', names_from = 'task_id', values_from = 'classif.acc')

```
  
```{r}
glimpse(dt)
```


